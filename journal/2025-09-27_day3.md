# Journal — 27 Sep 2025 — Data Modeling (part 2)

## 1) What I learned (bullets, not prose)
- Dimensional Modeling
- Data Pipeline Process: Raw -> Clean -> Mart
- Normalization 
    * is done before the ELT
    * applied for transactional databases (e.g., POS, banks)
    * use to check data quality issues 
        * examples: number of rows between raw, clean andmart or aggregated computation if the result are the same
- Star Schema
    * use when you do a report
    * used by end-users
    * always put in Mart
- Snowflake vs Star Schema
    * A star schema: 
        * joins fact to dim tables
            * fact table - numerical or computational
            * dim table - other data that describes the report
    * A Snowflake schema
        * joins dim to dim tables
        * no fact table involved

## 2) New vocabulary (define in your own words)
- **Snowflake** - a schema used for joining different dimension tables.
- **Star schema** - has 1 fact table and many dimension tables
- **Fact Table** - the center table that contains the source of truth and is being linked to different dimension tables that expands the truthfulness of the data.
- **Dimnension Table** - these are tables carrying additional information that support the main information from the fact table.

## 3) Data Engineering mindset applied (what principles did I use?)
- SST: Single Source of Truth
    * Even if it's a small dataset, as a good practice, it should be scalable and follows the standardization.

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- 

## 5) Open questions (things I still don’t get)
- 

## 6) Next actions (small, doable steps)
- [ ] 

## 7) Artifacts & links (code, queries, dashboards)
- https://archive.ics.uci.edu/dataset/349/open+university+learning+analytics+dataset 

---

### Mini reflection (3–5 sentences)
What surprised me? What would I do differently next time? What will I watch out for in production?

### BONUS: What is a meme that best describes what you feel or your learning today?
